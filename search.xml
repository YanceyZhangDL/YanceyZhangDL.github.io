<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[[译]Show, attend and tell: 深度学习进行语义理解的的细节[下]]]></title>
      <url>%2F2017%2F03%2F22%2FShow-attend-and-tell-%E6%9C%80%E5%85%88%E8%BF%9B%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E7%94%9F%E6%88%90%E5%9B%BE%E5%83%8F%E6%A0%87%E6%B3%A8%E7%9A%84%E7%BB%86%E8%8A%82-%E4%B8%8B%2F</url>
      <content type="text"><![CDATA[你们是如何与外星人交流的?如何将自己的语言教会他们?我们教孩子们学习的方法正确吗? 向量空间在继续向下讲之前，需要提前解释一个在机器学习中非常重要的概念。通常，我们要将所有输入（图像，声音，单词等）都表示为向量，来方便后续的处理。你也可以将这个长度为n的向量看做n维空间中的一个点。但是为什么要将图像和单词用向量表示？因为我们理解向量空间，我们理解向量空间中的距离。例如。我们知道点（1,0,0）比三维空间中的点（0,5,0）更接近点（0,0,0）。如果我们可以在较高维度空间中用点表示两个单词，我们可以通过计算这些点在较高维度空间中的距离，来获取它们在意义或上下文中的相似程度。 从概念上理解，将图像和单词表示为较高维空间中的点，听起来不错。困难的部分是弄清楚如何做到这一点。 通过提取任何现有流行的卷积网络结构的倒数第二层（分类层之前）输出，可以将图像表示为特征向量。比如VGG-net，GoogleNet等。 对于单词，我们希望这种表示能够在单词的意义上相近或者上下文可以落在相近的向量空间中。将单词转化为向量的方法称作word2vec，这是如今比较流行的一类方法。word2vec的灵感来自于这句著名的谚语：You shall know the word from the company it keeps”，通过这句话所传达的意义来学习词汇的表示。 word2vec的学习通过一个与我们工作的领域相关的语料库来完成。我们以域“computer vision”为例。另\(w_{1}, w_{2}…w_{M}\)为语料库中不重复的唯一的单词，\(S_{w_1},S_{w_2}…S_{w_M}\)为对应的我们需要寻找的单词向量。为了确定单词向量，word2vec训练一个可以预测给定单词的上下文单词的系统。上下文单词被定义为在该单词两边的给定大小的窗口（这里是2）中出现的单词。比如，将如下的三句话当成我们的语料库。（这只是为了演示的目的。在真实的应用程序中，语料库应该非常大，以获得有意义的表示。） This blog is about deep learning and computer vision. We love deep learning. We love computer vision. 给定单词”computer”，我们的任务是从第一句话中学习并预测上下文单词”learning”，”and”和”vision”并且从最后一句话中学习预测”we”，”love”和”vision”。因此，训练目标是使得给出“computer”一词的这些上下文单词的对数概率最大化。公式如下： $$Objective = Maximize\sum _{t=1}^{T}\sum _{-m \geq j \geq m}\log P\left( s_{w_{t}+j}|S_{w_t}\right)$$ 上式中，m为上下文窗口大小(这里是2)，t将整个语料库的单词循环一遍（在这里是将所有句子的单词循环一遍）。 \( P(S_{w_{t+j} | S_{w_t}})\)由上下文向量和中心词向量之间的相似度或內积来建模。对于每个单词，都有两个向量与之相关联。当它作为上下文或中心词时分别由R和S来表示。 所以\( P(S_{w_{t+j} | S_{w_t}})\)被看做\( \frac {e^{R^T_{w_{t+j}}}} {\sum _{i=1}^{M} e^{R^T_i S_{w_t}}} \)。这里分母是将中心词向量与词汇表中每个其他单词的上下文向量的相似度进行归一化，使得概率相加和为1。 将每个单词的中心向量当做其向量表达，并同RNN在公式3中使用。 真实世界图片中的噪声处理：注意力机制当场景混乱时，简单的系统就很难生成复杂的图像描述。我们，人类，通过一睹图像的全局来排除干扰。当我们继续描述图像时，我们的焦点将转移到图像中的不同区域。对于计算机，已经提出了类似的注意机制来模拟人的行为。这种注意机制只允许图像中的重要特征在需要时出现在我们面前。在每个步骤中，确定图像的显着区域并将其馈送到RNN中，而不是使用来自整个图像的特征。系统从图像中获得关注的视图，并预测与该区域相关的单词。需要基于之前生成的单词来确定此步需要关注的图像区域。否则，新生成的单词可能与图像区域对应，但不会是我们的图像描述中所需要的。 到目前为止，图像卷积层全连接层的输出作为RNN网络的输入。该输出对应于整个图像。那么，我们如何获得一个只对应一个图像的小部分的特征？卷积层的输出编码本地信息，而不是图像混乱的原始信息。卷积层的输出是2d特征图，其中每个位置受到与卷积核大小（感受野）相对应的图像区域的影响。要理解这一点，请看图4，其展示了一个卷积神经网络的样子。在输出层之前，有一个完全连接的层，就像一个伸展开的向量，代表整个输入图像，而卷积层的输出(全连接层之前的所有层)更像多维的2d图像。在某个特定位置的单个特征图中通过所有维度提取的向量表示图像的一个局部区域特征。 在每一步中，我们需要确定特征图上与当前步相关的特征区域。相关位置的特征向量将被馈送到RNN。所以，我们基于先前生成的单词来生成各个位置上对应的概率分布。\(L_{t,i} = 1\)意味着第i个位置被用来在第t步中生成当前的词汇。让\(a_i\)为从卷积特征图中提取的第i个位置所对应的特征向量。 我们需要的值为： $$ p(L_{t, i} = 1 | I, S_0, S_1…S_{t-1} ) \approx p(L_{t, i} | h_t) = \beta_{t, i} \propto \alpha^T_i h_t $$ 在这里，选择位置\(\beta_{t, i}\)的概率被转化成正比于上式中的点乘，即该位置的特征向量与RNN隐藏向量的相似度。 现在在概率分布的基础上，可以使用与具有最大概率的特征位置相对应的特征向量进行预测，但是使用根据概率加权的所有位置向量的聚合向量可以使训练更简单快速地收敛。所以我们另\(z_t\)作为输入RNN的上下文或聚合向量。 $$ z_t = \sum _{i=1}^{n}\beta_{t, i}\alpha_i $$ 所以公式2变为 $$p(S_t | I, h_t) =&gt; p(S_t | z_t, h_t)$$ 因此，这种机制模拟人的行为，在描述图像的过程中将注意力集中在图像的不同位置，而且，通过这种集中的视角，可以为图像生成更加复杂的描述，甚至可以满足图像中更复杂的细节。图5展示了在文献1中的标注示例。 所有这些小部件就像现在最流行的图像标注系统的基岩。有许多图像标注领域的论文围绕着这些中心思想，而在这大多数论文中，大部分的不同或多或少的集中在以下几个方面： 单词的特征向量如何学习。(例如：GloVe便是另一种单词特征向量的生成方式) 反向传播是否应用于图像以及word2vec的训练之中。 使用LSTM(长短时记忆单元)来代替RNN，从而解决RNN中的梯度消失问题。 Stochastic vs soft attention: 在这里，我们讨论了Soft attention机制，其中不同的位置向量是根据其个体的概率进行聚合的。在stochastic mechanism中，基于概率分布对单个位置进行采样，并仅将采样位置的特征向量用于到RNN单元之中。 References Xu, J. Ba, R. Kiros, A. Courville, R. Salakhutdinov, R. Zemel, and Y. Bengio, Show, attend and tell: Neural image caption generation with visual attention Vinyals, A. Toshev, S. Bengio, and D. Erhan, Show and tell: A neural image caption generator]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[[译]Show, attend and tell: 深度学习进行语义理解的的细节[上]]]></title>
      <url>%2F2017%2F03%2F22%2FShow-attend-and-tell-%E6%9C%80%E5%85%88%E8%BF%9B%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E7%94%9F%E6%88%90%E5%9B%BE%E5%83%8F%E6%A0%87%E6%B3%A8%E7%9A%84%E7%BB%86%E8%8A%82-%E4%B8%8A%2F</url>
      <content type="text"><![CDATA[你们是如何与外星人交流的?如何将自己的语言教会他们?我们教孩子们学习的方法正确吗? 你们是如何与外星人交流的? 如何将自己的语言教会他们? 我们教孩子们学习的方法正确吗? 是的，我们通过将各种不同的物体展示给他们并告诉他们物体的名字来教会他们。 将图片和文字的信息结合起来, 这就是自动图像标注的目标。 这是一个宽泛而又棘手的问题，为此前人已经做出了几十年的努力。仅凭一篇博客是无法将这一领域的问题全部归纳清楚的，就像仅阅读标题和看这幅图片的内容是截然不同的。但是在这里，我们可以做出尝试。 如果你的目的仅仅是使用一个目前通用的图像标注系统，那么事情变得很简单了，因为Google已经有基于Tensorflow的开源的图像标注代码im2txt。如果你使用Torch/lua，那么Karpathy的NeuralTalk2对你来说是一个入门的好开端，他们的代码也已经在Github上面开源了。但是你必须要知道的是，Google在这方面已经做了大量的工程实践来使得系统适合实际的应用，所以基于TensorFlow的实现更推荐你来使用。 在这篇博客中，我们将讨论现阶段图像标注领域中涉及到的最先进的理论以及应用基础。尽管在许多领域计算机几乎已经达到了与人类齐平的表现(e.g. 图像分类，语音识别，人脸识别)。然而，在自动图像标注领域，尽管已经有了几十年的研究，但是标注水平距离人类还差得很远。但是深度学习在物体识别，定位以及细粒度属性识别领域的高速发展对图像标注问题的准确率带来了前所未有的提升。早期的方法涉及通过图像检索来发现语义相似的图像，从索引数据库传送最相关的描述。但是显而易见，他们的应用范围是十分受限的。 目前的图像标注方法可以被分成三种： 第一类方法检测对象和属性，然后从包含那些对象的短语中合并图像的描述。 第二类方法将图像和对应的标注嵌入在相同的向量空间中。对于给定的查询，检索最接近嵌入空间中的图像的标注，并且修改这些标注以生成给定图像的新标注。然而，这些方法无法产生给定测试图像的新描述，因为来自最相似图像的描述用于突出标注。 第三类方法直接生成以输入图像和先前生成的词汇为条件的图像标注序列。因此，它们可以产生在训练数据中可能从未出现的词的新颖组合。这已经成为图像标注领域中最先进的算法中经常采用的标准方法。下面我们将着重阐述这种方法。 循环神经网络(RNN)为解锁方法对于图像标注任务，模型被用于对给定的图像产生正确的标注序列。这可以被转化成寻找使得如下对数概率获得最大值的标注序列： $$\log p\left( S | I\right) =\sum _{t=0}^{N}\log p\left( S_{t}|S_{0},S_{1},\ldots S_{t-1}\right) \tag{公式 1}$$ 在上式中： S 是最终生成的标注， \( S_{t} \)是在位置t的标注(\(t_{th}\) word) 输出单词的概率取决于先前生成的单词和输入图像，因此在方程中对这些变量进行调节。训练数据由数千个图像和与每个图像相关联的标注组成。训练数据中的标注由人手动书写。训练阶段包括找到在训练集中使得图像的标注概率最大化的模型参数。 在概率图模型中，对于这种密集连接的节点变量，其中每个节点依赖于所有其他节点，由于指数级的可能的配置数目，训练在计算上变得不可行。循环神经网络（RNN）提供了一种使用固定大小的隐藏向量来执行对先前变量的调节的平滑方法。 隐藏向量的作用如同前馈神经网络一样，被用来预测下一个输出的单词。 $$p\left( S_{t}|S_{0},S_{1},\ldots S_{t-1}\right) = p\left(S_{t}|I, h_{t} \right) \tag{公式 2}$$ 所以在步骤t，可变数量节点的复杂条件被一个简单的向量\(h_{t}\)所取代。然后为了做出在时间t的预测，上式用一个全连接层来对所有单词的概率分布进行建模， 全连接层输出的向量长度为词汇表中单词的数量，紧接着输出为一个全连接层。所以 \( p\left(S_{t}|I, h_{t}\right) \)变成 $$p\left(S_{t}|I, h_{t} \right) = softmax\left(L_{h}h_{t}+L_{I}I\right) \tag{公式 3}$$ 上式中\(L_{h}\)和\(L_{i}\)是全连接层的权重矩阵，输入作为\(h_{t}\)和\(I\)的一个级联向量。 根据输出的概率分布，拥有最大概率的单词被当做标注的下一个单词。在\(t+1\)步中，先前生成的单词的条件也要将新生成的单词\((S_{t})\)包含进去。但是RNN向量\((h_{t})\)以单词\(S_{1}, S_{2}…S_{t-1}, S_{t}\)为条件。所以\(S_{t}\) 通过一个线性层与\(h_{t}\) 结合，紧接着是一个非线性激励函数来计算\(h_{t+1}\)。这就是RNN单元，它包括了如下的计算： $$h_{t+1} = tanh(W_{h}h_{t}+W_{s}S_{t}) \tag{公式 4}$$ 整体的算法流程图如图2所示。 由于RNN类似于由线性层和非线性层组合而成的前馈神经网络，所以训练期间的反向传播是直接的，不需要太多的推理计算。与正常神经网络的唯一区别在于，通过在每个步骤中的相同参数集来完成先前的隐藏向量和新产生的单词的组合。这相当于将输出馈送到与输入相同的网络，因此网络的名称为循环神经网络。这样避免了网络大小的激增，否则在每一个步骤都需要一组新的参数。RNN单元可以如图3代表所示。 未完待续。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[个人博客搭建全纪录(Github Pages + Hexo + 个人域名绑定)]]></title>
      <url>%2F2017%2F03%2F15%2F%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E5%85%A8%E7%BA%AA%E5%BD%95%2F</url>
      <content type="text"><![CDATA[一、环境准备 二、Github Pages搭建 三、Hexo环境搭建并绑定Github Pages 1. 安装 2. 主题切换 3. 绑定Github Pages并发布 4. 写一篇新的文章 四、个人域名绑定(可选) 参考文章 耗时一天，踩了许多坑之后，自己的独立博客终于算是搭建完成了。前后了解了Jekyll和Hexo的搭建过程，发现Jekyll并不适合没有什么网页经验的自己，而Hexo全命令行的操作更容易上手。虽然现有的主题个人感觉没有特别中意的，不过最终还是选择了Github上面星星最多的Next。此主题简洁至上，比较适合不喜欢花哨的人群。而且此主题有比较详细的官方使用文档。所以如果你纠结于选择什么主题而抓狂的时候，NEXT是一个不错的尝试。曾经也考虑过使用云主机搭建一个完全独立的博客，但考虑到自己是穷逼学生，而Github的博客有免费的300M空间，何乐而不为呢？ 最后为了避免后面的同学踩坑，将搭建博客的所有细节记录如下。 一、环境准备首先声明，本人是在Linux系统上进行博客环境的搭建。 – 1. 首先需要一个Github Pages,当前前提是你要注册一个Github的账号 – 2. 本地配置Hexo开发环境(Hexo和Jekyll的作用一样，主要是省去我们完全自己部署博客的时间，不但提供了好看的主题，而且与Github Pages结合可以简便快捷的管理博客，博客编辑使用Markdown格式，同样是现在的主流)。 – 3. 绑定自己的独立域名(当然也可以不绑定，github pages有默认的二级域名可以使用，个人独立域名如果是学生的话推荐Name Cheap，认证后.me后缀的域名有免费的一年使用权) 二、Github Pages搭建Github Pages是一个免费的静态博客站点，特点包括：免费托管、主题和页面的高度可定制化。 – 1. 首先注册一个Github账号，有的同学跳过此步就好了。 – 2. 最好的搭建教程当然是Github Pages的官方教程 – 3. 一些细节问题还是在这里说一下：123451. 首先，登录Github, 在首页右下角，点击(New repository)2. RepositoryName的格式：yourusername.github.io，其中yourusername就是你的Github的用户名，记住，必须是自己的用户名！！！ 其他默认即可，不需要勾选生成README文件。3. 接下来按照官方教程操作就可以得到一个原始的博客页面了。 三、Hexo环境搭建并绑定Github PagesHexo是一个基于Node.js的静态博客框架，他是一个快速、简洁、高效的博客框架。 Hexo的四大特点： 极速生成静态页面 支持Markdown 一键部署博客 丰富的插件支持 1. 安装当然安装Hexo最好的还是参考官方中文文档。其中本地需要提前安装Git以及Node.js环境。安装好Git以及Node.js之后，下面命令安装Hexo:1npm install hexo-cli -g 本地博客环境安装以及初始化：123hexo init blog #blog就是新建博客目录的名字，可以随便起cd blognpm install 安装完成之后执行下面命令打开本地服务器可以查看博客效果, 浏览器输入localhost:4000，即可看到本地页面效果。1hexo server 建站的问题同样参考官方文档。 其中博客的目录结构如下1234567~ tree -L 1 blogblog├── _config.yml├── package.json├── scaffolds├── source└── themes 其中hexo的配置文件是 _config.yml ，可以在官方文档配置里查看详细解释。 2. 主题切换默认的主题可能不是很喜欢，我们可以在官方的主题页面查看并安装自己喜欢的主题。以NEXT为例，首先下载主题文件：12$ cd your-hexo-site$ git clone https://github.com/iissnan/hexo-theme-next themes/next 然后在博客根目录的_config.yml文件中，找到theme字段，将其值求改为next:1theme: next 切换完成后，重新编译,看一下效果：123hexo cleanhexo ghexo s 接下来，如果你想到自己修改主题以及显示等效果，可以参考NEXT或对应主题的Github官方文档，来修改对应主题下的_config.yml文件。其中也详细的介绍了绑定各种第三方插件的全流程，just try it. 3. 绑定Github Pages并发布接下来绑定Github Pages。首先需要安装插件：1npm install hexo-deployer-git --save 接下来打开_config.yml，找到deploy字段，假如Github Pages信息，如下：1234deploy:type: gitrepo: git@github.com:YanceyZhangDL/YanceyZhangDL.github.io.gitbranch: master 把Repo换成你自己的Github Pages的提交代码的git地址就可以了。 接下来当然还需要将你的博客发布到Github Pages的服务器上：123hexo cleanhexo g #生成本地发布文件夹，g是generate的缩写hexo d #将博客部署到Github Pages, d是deploy的缩写 其他快捷键常用组合：12$ hexo d -g #生成部署$ hexo s -g #生成预览 这个时候，其实你的独立博客已经搭建完成了。 4. 写一篇新的文章hexo new “new article” 打开之后我们会看到：12345---title: new articledate: 2014-11-01 20:10:33tags:--- 文件的开头是属性，采用统一的yaml格式，文章正文支持Markdown。 四、个人域名绑定(可选)由于笔者还是学生，针对学生Github有一个学生包的优惠，申请下来之后可以在Name Cheap申请免费域名，有一年的免费使用权，或者你可以使用国内的万网或者国外的GoDaddy等都可以。 说一下域名刚申请下来之后绑定的流程，本文以DNSPod解析为例，各大网站注册域名在DNSPod解析的帮助文档： – 1. 首先需要在注册的网站进行DNS短地址的修改 – 2. 然后在本地站点的source目录下添加一个CNAME文件，文件中加入你的域名，例如我的是：yanceyzhang,me，不需要加www，也不需要添加http://。添加完成后重新部署(hexo d -g) – 3. 然后注册DNSPod，添加域名，添加记录。12345671. 添加域名填写你的域名，不用添加http://，然后在点击你的域名点进去在添加记录（其中记录中CHAME的值是你的github pages的地址）。2. 再添加两条A记录：@ A 默认 192.30.252.154@ A 默认 192.30.252.1533. 启用就可以了。 – 4. 最后把本地的Hexo文件重新部署发布到Github Pages(hexo d -g)，打开你的域名，奇迹发生了～ 作者的个人博客地址：yanceyzhang.me，欢迎踩踏～ 参考文章– 1. Hexo官网：https://hexo.io/– 2. Hexo部署：http://www.tuicool.com/articles/m6fE32– 3. Hexo写一篇新的文章：http://www.cnblogs.com/fengsehng/p/6050527.html– 4. 博客是如何搭建的(github pages+HEXO+域名绑定)：http://www.jianshu.com/p/834d7cc0668d – 5. 手把手教你使用Hexo：https://linghucong.js.org/2016/04/15/2016-04-15-hexo-github-pages-blog/]]></content>
    </entry>

    
  
  
</search>
